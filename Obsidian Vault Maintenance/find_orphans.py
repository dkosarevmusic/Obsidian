import os
import re
import collections
import json
import time
import hashlib
import sys
from pathlib import Path
from urllib.parse import unquote

# PyYAML is required for advanced frontmatter parsing (e.g., in 'banner' property).
try:
    import yaml
except ImportError:
    yaml = None # Make it optional, check for it later.

# ================== CONFIGURATION ==================
# –£–∫–∞–∂–∏—Ç–µ –ê–ë–°–û–õ–Æ–¢–ù–´–ô –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ö—Ä–∞–Ω–∏–ª–∏—â—É Obsidian
# –ü—Ä–∏–º–µ—Ä –¥–ª—è Windows: "C:/Users/User/Documents/MyVault"
# –ü—Ä–∏–º–µ—Ä –¥–ª—è macOS/Linux: "/home/user/MyVault". –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å / –≤–º–µ—Å—Ç–æ \.
VAULT_PATH = "C:/Obsidian/dkosarevmusic"

REPORT_FILE_NAME = "orphans_report.md"
CACHE_FILE_NAME = ".find_orphans_cache.json"

# –ü–∞–ø–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–∏.
# –£–∫–∞–∑—ã–≤–∞–π—Ç–µ –ø—É—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ—Ä–Ω—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞, –∏—Å–ø–æ–ª—å–∑—É—è '/' –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è.
# –ù–∞–ø—Ä–∏–º–µ—Ä: ["_templates", "resources/attachments"]
IGNORED_FOLDERS = [
    "Excalidraw", "Templates", "JS Scripts"
]
# –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ª–∏ —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (–Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–ø–∞–ø–∫–∏).
# True - –¥–∞, –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–Ω–µ. False - –Ω–µ—Ç, —Å–∫–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω–æ.
IGNORE_ROOT_FILES = True
# ===================================================

# –°–≤–æ–π—Å—Ç–≤–∞ –≤ frontmatter, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Å—ã–ª–∫–∏ –≤ –≤–∏–¥–µ –ø—Ä–æ—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞.
# –°–∫—Ä–∏–ø—Ç –±—É–¥–µ—Ç —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ —ç—Ç–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –∫–∞–∫ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Ñ–∞–π–ª.
# –ü—Ä–∏–º–µ—Ä: `banner: "attachments/image.png"`
FRONTMATTER_LINK_PROPERTIES = [
    "banner",
    "image",
]

# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —Å–∫—Ä–∏–ø—Ç—É) ---

# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å—Å—ã–ª–æ–∫
WIKI_RE = re.compile(r'\[\[([^\]\|#]+)')
INLINE_RE = re.compile(r'\[.*?\]\(([^)\s#?]+)')
# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ "–≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö" —Å—Å—ã–ª–æ–∫
QUERY_BLOCK_RE = re.compile(r'```(?:dataview|base).*?\n(.*?)\n```', re.DOTALL | re.IGNORECASE)
# –ò—â–µ—Ç `wikilinks.contains(link("..."))` –∏ `wikilinks.contains(link(this.file.name))`
DATAVIEW_FILTER_RE = re.compile(r'wikilinks\.contains\(link\((?:"([^"]+)"|this\.file\.name)\)\)')
FRONTMATTER_RE = re.compile(r'^---\s*\n(.*?)\n---', re.DOTALL)
FM_WIKILINKS_SECTION_RE = re.compile(r'^wikilinks:(.*?)(?=\n^\S|\Z)', re.MULTILINE | re.DOTALL)
FM_WIKILINK_ITEM_RE = re.compile(r'\[\[([^\]\|#]+)\]\]')

def _get_script_hash() -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à SHA256 —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫—ç—à–∞."""
    try:
        with open(__file__, 'rb') as f:
            script_content = f.read()
        return hashlib.sha256(script_content).hexdigest()
    except Exception:
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –Ω–∞ —Å–ª—É—á–∞–π –æ—à–∏–±–∫–∏ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—â–∏–π –ø–æ–≤—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑.
        return str(time.time())


def build_file_index(vault_path: Path, ignored_folders: list[str], cache_file_name: str, report_file_name: str, ignore_root_files: bool) -> dict[str, Path]:
    """
    –°–æ–∑–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫.
    –ö–ª—é—á - –∏–º—è —Ñ–∞–π–ª–∞ (e.g., 'My Note.md'), –∑–Ω–∞—á–µ–Ω–∏–µ - –ø–æ–ª–Ω—ã–π –ø—É—Ç—å (Path object).
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç os.scandir –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    """
    index = {}
    ignored_folders_set = {p.strip().lower().replace("\\", "/") for p in ignored_folders}
    ignored_files_set = {cache_file_name, report_file_name}

    for root, dirs, files in os.walk(vault_path, topdown=True):
        root_path = Path(root)

        # –û—Ç—Å–µ–∫–∞–µ–º –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ –∏ —Å–∫—Ä—ã—Ç—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏–∑ –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—Ö–æ–¥–∞
        dirs[:] = [
            d for d in dirs if not d.startswith('.') and
            (root_path / d).relative_to(vault_path).as_posix().lower() not in ignored_folders_set
        ]

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –≤ –∫–æ—Ä–Ω–µ–≤–æ–π –ø–∞–ø–∫–µ, –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–∞ –æ–ø—Ü–∏—è
        if ignore_root_files and root_path == vault_path:
            continue

        for file_name in files:
            if file_name.startswith('.') or file_name in ignored_files_set:
                continue
            index[os.path.normcase(file_name)] = root_path / file_name

    return index

def find_file_in_vault(file_index: dict[str, Path], file_name: str) -> Path | None:
    """–ò—â–µ—Ç —Ñ–∞–π–ª –≤ –∏–Ω–¥–µ–∫—Å–µ, –ø—Ä–æ–±—É—è —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω–∏."""
    path = file_index.get(os.path.normcase(file_name))
    if path:
        return path
    if not file_name.lower().endswith('.md'):
        path = file_index.get(os.path.normcase(f"{file_name}.md"))
        if path:
            return path
    return None

def _clean_markdown_body(body: str) -> str:
    """
    –£–¥–∞–ª—è–µ—Ç –±–ª–æ–∫–∏ –∫–æ–¥–∞ –∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫ –∏–∑ –Ω–∏—Ö.
    """
    cleaned_body = re.sub(r'```.*?```', '', body, flags=re.DOTALL)
    cleaned_body = re.sub(r'%%.*?%%', '', cleaned_body, flags=re.DOTALL)
    return re.sub(r'`[^`]*`', '', cleaned_body)

def _create_obsidian_link(file_path: Path, vault: Path) -> str:
    """–°–æ–∑–¥–∞–µ—Ç –∫–ª–∏–∫–∞–±–µ–ª—å–Ω—É—é wikilink-—Å—Å—ã–ª–∫—É –¥–ª—è Obsidian."""
    relative_path_str = file_path.relative_to(vault).as_posix()
    if relative_path_str.lower().endswith('.md'):
        link_text = relative_path_str[:-3]
    else:
        link_text = relative_path_str
    return f"[[{link_text}]]"

def _generate_table_for_category(files: list[Path], vault: Path) -> list[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Markdown-—Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ñ–∞–π–ª–æ–≤, —Å–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ –ø–∞–ø–∫–∞–º."""
    lines = []
    grouped_files = collections.defaultdict(list)
    for path in files:
        grouped_files[path.parent].append(path)

    for folder_path in sorted(grouped_files.keys(), key=str):
        folder_name = folder_path.relative_to(vault).as_posix()
        if folder_name == ".":
            folder_name = "/"
        
        lines.append(f"### üìÅ `{folder_name}`\n")
        lines.append("| –§–∞–π–ª | –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ | –†–∞–∑–º–µ—Ä |\n")
        lines.append("|:---|:---|:---|\n")

        sorted_files_in_folder = sorted(
            grouped_files[folder_path], 
            key=lambda p: (p.suffix.lower(), p.name.lower())
        )

        for file in sorted_files_in_folder:
            link = _create_obsidian_link(file, vault)
            ext = file.suffix[1:].lower() if file.suffix else ""
            size_kb = f"{(file.stat().st_size / 1024):.1f} KB"
            lines.append(f"| {link} | `{ext}` | `{size_kb}` |\n")
        
        lines.append("\n")
    return lines

def _format_dead_ends_with_loose_ends(items: dict, vault: Path) -> list[str]:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ '—Ç—É–ø–∏–∫–∏ —Å –æ–±–æ—Ä–≤–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏'."""
    lines = []
    sorted_items = sorted(items.items(), key=lambda item: str(item[0]))
    for file_path, broken_links in sorted_items:
        lines.append(f"- {_create_obsidian_link(file_path, vault)}\n")
        for broken in sorted(broken_links):
            lines.append(f"  - `(–±–∏—Ç–∞—è —Å—Å—ã–ª–∫–∞) ‚Üí {broken}`\n")
    lines.append("\n")
    return lines

def _extract_strings_from_yaml_value(value) -> list[str]:
    """Recursively extracts all string values from a nested YAML structure (lists/dicts)."""
    strings = []
    if isinstance(value, str):
        strings.append(value)
    elif isinstance(value, list):
        for item in value:
            strings.extend(_extract_strings_from_yaml_value(item))
    elif isinstance(value, dict):
        for sub_value in value.values():
            strings.extend(_extract_strings_from_yaml_value(sub_value))
    return strings

def analyze_file_content(content: str, file_path: Path, vault_path: Path, file_index: dict[str, Path]) -> dict:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞, –∏–∑–≤–ª–µ–∫–∞—è –≤—Å–µ –≤–∏–¥—ã —Å—Å—ã–ª–æ–∫ –∑–∞ –æ–¥–∏–Ω –ø—Ä–æ—Ö–æ–¥."""
    valid_links = set()
    broken_links = set()
    has_external_links = False
    fm_wikilinks = []
    dataview_hubs_found = []

    # --- 1. –ê–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –≤ —Ç–µ–ª–µ –∏ frontmatter (WIKI –∏ INLINE) ---
    # _clean_markdown_body —É–¥–∞–ª—è–µ—Ç –∫–æ–¥-–±–ª–æ–∫–∏ –∏ —Ç.–¥., –≥–¥–µ —Å—Å—ã–ª–∫–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è.
    cleaned_content = _clean_markdown_body(content)

    for match in WIKI_RE.finditer(cleaned_content):
        raw_link = match.group(1).strip()
        decoded_link = unquote(raw_link)
        target_path = find_file_in_vault(file_index, decoded_link)
        if target_path and target_path.exists():
            valid_links.add(target_path)
        else:
            broken_links.add(decoded_link)

    for match in INLINE_RE.finditer(cleaned_content):
        raw_link = match.group(1).strip()
        decoded_link = unquote(raw_link)
        if decoded_link.startswith(('http://', 'https://', 'ftp://', 'mailto:')):
            has_external_links = True
            continue
        current_dir = file_path.parent
        potential_path = (current_dir / decoded_link).resolve()
        if potential_path.exists() and potential_path.is_file():
            valid_links.add(potential_path)
            continue
        file_name_only = Path(decoded_link).name
        target_path = find_file_in_vault(file_index, file_name_only)
        if target_path and target_path.exists():
            valid_links.add(target_path)
        else:
            broken_links.add(decoded_link)

    # --- 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Frontmatter ---
    fm_match = FRONTMATTER_RE.match(content)
    if fm_match:
        frontmatter_content = fm_match.group(1)
        
        # 2.1. –ê–Ω–∞–ª–∏–∑ "–≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö" —Å—Å—ã–ª–æ–∫ –¥–ª—è dataview (`wikilinks` property)
        for section in FM_WIKILINKS_SECTION_RE.finditer(frontmatter_content):
            for link_match in FM_WIKILINK_ITEM_RE.finditer(section.group(1)):
                hub_name = link_match.group(1).strip()
                fm_wikilinks.append(hub_name)
        
        # 2.2. –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–≤–æ–π—Å—Ç–≤–∞—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º YAML-–ø–∞—Ä—Å–µ—Ä–∞
        if yaml and FRONTMATTER_LINK_PROPERTIES:
            try:
                fm_data = yaml.safe_load(frontmatter_content)
                if isinstance(fm_data, dict):
                    for prop in FRONTMATTER_LINK_PROPERTIES:
                        if prop in fm_data:
                            # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ —Å–≤–æ–π—Å—Ç–≤–∞
                            link_candidates = _extract_strings_from_yaml_value(fm_data[prop])
                            
                            for raw_link_text in link_candidates:
                                link_text = ""
                                # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–∏–∫–∏-—Å—Å—ã–ª–∫–æ–π
                                wiki_links_in_value = FM_WIKILINK_ITEM_RE.findall(raw_link_text)
                                if wiki_links_in_value:
                                    link_text = wiki_links_in_value[0].strip()
                                else:
                                    # –ï—Å–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—é —Å—Ç—Ä–æ–∫—É –∫–∞–∫ –µ—Å—Ç—å
                                    link_text = raw_link_text.strip()

                                if not link_text:
                                    continue
                                
                                decoded_link = unquote(link_text)
                                
                                if decoded_link.startswith(('http://', 'https://')):
                                    has_external_links = True
                                    continue

                                # –õ–æ–≥–∏–∫–∞ –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–∞ (—Ç–∞–∫–∞—è –∂–µ, –∫–∞–∫ –±—ã–ª–∞)
                                # 1. Try as a path relative to the vault root.
                                potential_path = (vault_path / decoded_link).resolve()
                                if potential_path.exists() and potential_path.is_file():
                                    valid_links.add(potential_path)
                                    continue

                                # 2. If not, try to find by filename in the whole vault (fallback) and handle broken links
                                file_name_only = Path(decoded_link).name
                                target_path = find_file_in_vault(file_index, file_name_only)
                                if target_path and target_path.exists():
                                    valid_links.add(target_path)
                                else:
                                    broken_links.add(decoded_link)
            except yaml.YAMLError:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞ YAML, —á—Ç–æ–±—ã –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞—Ç—å –≤–µ—Å—å –∞–Ω–∞–ª–∏–∑
                pass

    # --- 3. –ê–Ω–∞–ª–∏–∑ dataview-–∑–∞–ø—Ä–æ—Å–æ–≤ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≥–¥–µ —É–≥–æ–¥–Ω–æ –≤ —Ñ–∞–π–ª–µ) ---
    for block in QUERY_BLOCK_RE.finditer(content):
        for match in DATAVIEW_FILTER_RE.finditer(block.group(1)):
            hub_name = match.group(1).strip() if match.group(1) else file_path.stem
            dataview_hubs_found.append(hub_name)

    return {
        "valid_links": sorted([p.relative_to(vault_path).as_posix() for p in valid_links]),
        "broken_links": sorted(list(broken_links)),
        "has_external_links": has_external_links,
        "fm_wikilinks": fm_wikilinks,
        "dataview_hubs": dataview_hubs_found,
    }


def _load_cache(cache_path: Path) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à –∏–∑ —Ñ–∞–π–ª–∞, –µ—Å–ª–∏ –æ–Ω —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –≤–∞–ª–∏–¥–µ–Ω."""
    if not cache_path.exists():
        return {}
    try:
        current_hash = _get_script_hash()
        with open(cache_path, 'r', encoding='utf-8') as f:
            cache_data = json.load(f)
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ö—ç—à —Å–∫—Ä–∏–ø—Ç–∞. –ï—Å–ª–∏ –æ–Ω –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç, —Å—á–∏—Ç–∞–µ–º –∫—ç—à –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–º.
        if cache_data.get("script_hash") != current_hash:
            print("  ‚ÑπÔ∏è  –õ–æ–≥–∏–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞ –∏–∑–º–µ–Ω–∏–ª–∞—Å—å. –ë—É–¥–µ—Ç –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
            return {}
        return cache_data.get("files", {}) # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–ª–æ–≤–∞—Ä—å —Å —Ñ–∞–π–ª–∞–º–∏
    except (json.JSONDecodeError, IOError):
        print("  ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫—ç—à, –±—É–¥–µ—Ç –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑.")
        return {}

def _save_cache(cache_path: Path, cache_data: dict):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–∞–π–ª –∫—ç—à–∞."""
    full_cache_content = {
        "script_hash": _get_script_hash(),
        "files": cache_data
    }
    try:
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump(full_cache_content, f, indent=2)
        print(f"‚úÖ –ö—ç—à —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {cache_path}")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫—ç—à: {e}")

def _analyze_all_files(
    markdown_files: list[Path], vault_path: Path, file_index: dict, cache: dict
) -> tuple[dict[Path, dict], dict]:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ markdown-—Ñ–∞–π–ª—ã, –∏—Å–ø–æ–ª—å–∑—É—è –∫—ç—à."""
    print("üîÑ –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ (—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞)...")
    new_cache = {}
    all_analysis_data = {}
    files_from_cache = 0
    files_analyzed = 0

    for md_file in markdown_files:
        file_key = md_file.relative_to(vault_path).as_posix()
        current_mtime = md_file.stat().st_mtime

        if file_key in cache and cache[file_key].get("mtime") == current_mtime:
            all_analysis_data[md_file] = cache[file_key]["analysis"]
            new_cache[file_key] = cache[file_key]
            files_from_cache += 1
        else:
            try:
                content = md_file.read_text(encoding='utf-8')
                analysis_result = analyze_file_content(content, md_file, vault_path, file_index)
                all_analysis_data[md_file] = analysis_result
                new_cache[file_key] = {"mtime": current_mtime, "analysis": analysis_result}
                files_analyzed += 1
            except Exception as e:
                print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ñ–∞–π–ª–∞ {md_file.name}: {e}")

    print(f"  - –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞: {files_from_cache} —Ñ–∞–π–ª–æ–≤.")
    print(f"  - –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–Ω–æ–≤–æ: {files_analyzed} —Ñ–∞–π–ª–æ–≤.")
    return all_analysis_data, new_cache

def _build_link_graph(all_files: list[Path], analysis_data: dict, vault_path: Path) -> dict:
    """–°—Ç—Ä–æ–∏—Ç –≥—Ä–∞—Ñ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ñ–∞–π–ª–∞–º–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞."""
    print("üîÑ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å—Å—ã–ª–æ–∫...")
    file_graph = {
        path: {"in_links": set(), "out_links": set(), "broken_out_links": set(), "has_external_links": False}
        for path in all_files
    }

    dataview_hubs = collections.defaultdict(list)
    dataview_link_targets = collections.defaultdict(list)

    for md_file, data in analysis_data.items():
        valid_links = {vault_path / p for p in data["valid_links"]}
        file_graph[md_file]["out_links"].update(valid_links)
        file_graph[md_file]["broken_out_links"].update(data["broken_links"])
        file_graph[md_file]["has_external_links"] = data["has_external_links"]
        for target_path in valid_links:
            if target_path in file_graph:
                file_graph[target_path]["in_links"].add(md_file)

        for hub_name in data["dataview_hubs"]:
            dataview_hubs[hub_name.lower()].append(md_file)
        for hub_name in data["fm_wikilinks"]:
            dataview_link_targets[hub_name.lower()].append(md_file)

    virtual_links_count = 0
    for hub_name_lower, aggregator_files in dataview_hubs.items():
        if hub_name_lower in dataview_link_targets:
            target_files = dataview_link_targets[hub_name_lower]
            for aggregator_file in aggregator_files:
                for target_file in target_files:
                    file_graph[target_file]["in_links"].add(aggregator_file)
                    virtual_links_count += 1

    if virtual_links_count > 0:
        print(f"  - –£—á—Ç–µ–Ω–æ {virtual_links_count} '–≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö' —Å—Å—ã–ª–æ–∫.")
    print("‚úÖ –ì—Ä–∞—Ñ —Å—Å—ã–ª–æ–∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω.")
    return file_graph

def _categorize_files(file_graph: dict, report_file_name: str) -> dict:
    """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Ñ–∞–π–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥—Ä–∞—Ñ–∞ —Å—Å—ã–ª–æ–∫."""
    print("üîÑ –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤...")
    categories = {
        "dead_ends_with_loose_ends": {},
        "dead_ends": [],
        "absolute_orphans": [],
    }

    for file_path, data in file_graph.items():
        if file_path.name == report_file_name:
            continue

        has_in_links = bool(data["in_links"])
        has_out_links = bool(data["out_links"])
        has_broken_links = bool(data["broken_out_links"])
        has_external_links = data["has_external_links"]

        if not has_in_links:
            if has_broken_links:
                categories["dead_ends_with_loose_ends"][file_path] = data["broken_out_links"]
            elif has_out_links or has_external_links:
                categories["dead_ends"].append(file_path)
            else:
                categories["absolute_orphans"].append(file_path)
    
    total_found = sum(len(v) for v in categories.values())
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {total_found} –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤.")
    return categories

def _generate_report(categories: dict, vault_path: Path, report_path: Path):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π markdown-–æ—Ç—á–µ—Ç."""
    total_found = sum(len(v) for v in categories.values())

    # –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ—Ç, —Å–æ–∑–¥–∞–µ–º —á–∏—Å—Ç—ã–π –æ—Ç—á–µ—Ç –∏ –≤—ã—Ö–æ–¥–∏–º.
    if total_found == 0:
        print("üéâ –í—Å—ë —Å–≤—è–∑–∞–Ω–æ ‚Äî –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ!")
        report_lines = [
            "---\ntags:\n  - optimization\n  - cleanup\n---\n\n",
            "# –û—Ç—á–µ—Ç –æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö\n\n",
            f"‚úÖ **–ü—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.**\n\n_–û—Ç—á–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω {time.strftime('%Y-%m-%d %H:%M:%S')}_"
        ]
        try:
            with open(report_path, 'w', encoding='utf-8') as f:
                f.writelines(report_lines)
            print(f"‚úÖ –û—Ç—á–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–±–ª–µ–º: {report_path}")
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞ –æ—Ç—á–µ—Ç–∞: {e}")
        return

    print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–∞...")
    report_lines = [
        "---\ntags:\n  - optimization\n  - cleanup\n---\n\n",
        f"# –û—Ç—á–µ—Ç –æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Ñ–∞–π–ª–∞—Ö ({total_found} —à—Ç.)\n\n"
    ]

    report_sections = {
        "dead_ends_with_loose_ends": {
            "title": "üï∏Ô∏è –¢—É–ø–∏–∫–∏ —Å –æ–±–æ—Ä–≤–∞–Ω–Ω—ã–º–∏ —Å—Å—ã–ª–∫–∞–º–∏",
            "description": "–§–∞–π–ª—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ—Ç —Å—Å—ã–ª–æ–∫, –∏ –∫–æ—Ç–æ—Ä—ã–µ —Å–∞–º–∏ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ **–Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ** —Ñ–∞–π–ª—ã. **–¢—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å.**",
            "formatter": _format_dead_ends_with_loose_ends
        },
        "dead_ends": {
            "title": "üõë –¢—É–ø–∏–∫–∏",
            "description": "–§–∞–π–ª—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ—Ç —Å—Å—ã–ª–æ–∫, –Ω–æ –∫–æ—Ç–æ—Ä—ã–µ —Å–∞–º–∏ —Å—Å—ã–ª–∞—é—Ç—Å—è –Ω–∞ **—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ** —Ñ–∞–π–ª—ã. –ù–∞ —ç—Ç–∏ —Ñ–∞–π–ª—ã –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –ø–æ–ø–∞—Å—Ç—å –ø–æ —Å—Å—ã–ª–∫–∞–º.",
            "formatter": _generate_table_for_category
        },
        "absolute_orphans": {
            "title": "üóëÔ∏è –ê–±—Å–æ–ª—é—Ç–Ω—ã–µ —Å–∏—Ä–æ—Ç—ã",
            "description": "–§–∞–π–ª—ã, —É –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –Ω–∏ –≤—Ö–æ–¥—è—â–∏—Ö, –Ω–∏ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫.",
            "formatter": _generate_table_for_category
        }
    }

    for key, config in report_sections.items():
        items = categories.get(key)
        if items:
            report_lines.append(f"## {config['title']} ({len(items)} —à—Ç.)\n\n")
            report_lines.append(f"{config['description']}\n\n")
            report_lines.extend(config['formatter'](items, vault_path))

    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            f.writelines(report_lines)
        print(f"\n‚úÖ –û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {report_path}")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ —Ñ–∞–π–ª–∞ –æ—Ç—á–µ—Ç–∞: {e}")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤-—Å–∏—Ä–æ—Ç."""
    start_time = time.time()
    vault = Path(VAULT_PATH)
    if not vault.is_dir():
        print(f"‚ùå –û—à–∏–±–∫–∞: –£–∫–∞–∑–∞–Ω–Ω—ã–π –ø—É—Ç—å –∫ —Ö—Ä–∞–Ω–∏–ª–∏—â—É –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø–∞–ø–∫–æ–π: {VAULT_PATH}")
        return

    if not yaml:
        print("\n  ‚ö†Ô∏è  –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ PyYAML –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (–∫–æ–º–∞–Ω–¥–∞ –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install PyYAML).")
        print("     –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å—Å—ã–ª–æ–∫ –≤ YAML-—Å–≤–æ–π—Å—Ç–≤–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'banner') –±—É–¥–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω.\n")

    # –®–∞–≥ 1: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
    print("üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Ñ–∞–π–ª–æ–≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
    file_index = build_file_index(vault, IGNORED_FOLDERS, CACHE_FILE_NAME, REPORT_FILE_NAME, IGNORE_ROOT_FILES)
    all_files = list(file_index.values())
    markdown_files = [f for f in all_files if f.suffix.lower() == '.md']
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_files)} —Ñ–∞–π–ª–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ ({len(markdown_files)} markdown).")

    # –®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫—ç—à–∞
    script_dir = Path(__file__).parent.resolve()
    cache_path = script_dir / CACHE_FILE_NAME
    cache = _load_cache(cache_path)
    analysis_data, new_cache = _analyze_all_files(markdown_files, vault, file_index, cache)
    _save_cache(cache_path, new_cache)

    # –®–∞–≥ 3: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ —Å—Å—ã–ª–æ–∫
    file_graph = _build_link_graph(all_files, analysis_data, vault)

    # –®–∞–≥ 4: –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤
    categories = _categorize_files(file_graph, REPORT_FILE_NAME)

    # –®–∞–≥ 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
    report_path = vault / REPORT_FILE_NAME
    _generate_report(categories, vault, report_path)
    
    end_time = time.time()
    print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {end_time - start_time:.2f} —Å–µ–∫.")


if __name__ == "__main__":
    main()